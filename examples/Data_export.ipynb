{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export with the lucullus REST API\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Hello dear reader! This example file shold guide you through all the steps necessary to export data through the Lucullus REST API using the \"lucullus_rest\" python library. With this you can easily export data without having to open Lucullus, and there are several reasons why you would like to do this:\n",
    "* Automation saves time: If you know exactly what you need, and if you always need the same data, it can save some time to automatically export your data, especially if you need to extract data of many processes. Setting up a script can be especially useful if you need to export data of many processes. Just imagine you forgot about exporting a signal, in which case you only need to update your script instead of manually extracting all the data on your own again.\n",
    "* Familiarizing yourself with how REST works will help you also with other interfaces for collecting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting process data by hand\n",
    "\n",
    "At first, to show how things work, we will get the data through the REST API step by step. For this we need to import the \"requests\" module to send and recieve data from the web. We will define our username and password so we can authenticate ourselfes and we want to get the pO2 signal from process PIP_555."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "auth = (\"user\", \"password\") # Change user and password!!!\n",
    "process = \"Process_555\"\n",
    "port_name = \"PV_Temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect this signal we need to get some more information. In Lucullus, every object is defined through a specific ID, so a process has a process ID, a reactor has a reactor ID, a port has a port ID and so on. At first we will find out what the process ID of our process is. For this we will create a URL that tells the REST API that we want to find a process which name is \"PIP_555\", and then we will get this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "rest_url = \"http://XXX.XXX.XXX.XXX:8080/lpims/rest/v1/\"\n",
    "\n",
    "request_link = rest_url + \"processes?name={}\".format(process)\n",
    "response = requests.get(request_link, auth=auth)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status code of our response should be \"200\" which tells us that our request was valid and that we could retrieve our information. From the response, we can get the information that we recieved in json format with the .json() command, which returns us a dictionary that we can navigate. From there we can get the information which process id defines our process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n"
     ]
    }
   ],
   "source": [
    "process_id = response.json()[\"data\"][0][\"id\"]\n",
    "print(process_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same we also have to do for our port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "request_link = rest_url + \"ports?name={}\".format(port_name)\n",
    "response = requests.get(request_link, auth=auth)\n",
    "\n",
    "port_id = response.json()[\"data\"][0][\"id\"]\n",
    "print(port_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know our process and port Id we can get our data. The sensor data is stored as a \"signal\" (which again has a dedicated signal ID, you get the point). A signal is the data of a specific process for a specific port. We can get the signals by telling the REST API to search for signals by matching process and port ID. From there on we get again a json file, from which we can get the time-value pairs as a loooong list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0014952180555555556, 73.20916748046875], [0.002919173055555555, 73.23304748535156], [0.004308261388888889, 73.68672180175781], [0.005727940833333334, 73.20916748046875], [0.007117025, 73.18528747558594], [0.00850617, 73.63896942138672], [0.009936127777777777, 73.80611419677734], [0.01132524222222222, 80.39636993408203], [0.012714335277777778, 76.12225341796875], [0.014103432777777777, 75.62081909179688]]\n"
     ]
    }
   ],
   "source": [
    "request_link = rest_url+\"signals?processId={}&portId={}\".format(process_id, port_id)\n",
    "response = requests.get(request_link, auth=auth)\n",
    "\n",
    "signal_data = response.json()[\"data\"][\"values\"]\n",
    "print(signal_data[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the internal lucullus_rest module\n",
    "\n",
    "Of course, to get the signal data in this way required a bit work and might not be very convenient for day to day use. As a consequence we created a Python module named lucullus_rest that tries to make things easier for us. From this module we can import the function export_to_df which takes a process name, a list of port names and authentication information as input, and returns to us a pandas dataframe with all the data. The data is in the same format if you would have exported the process data as an excel file through the graphic tool, which means that existing workflows do not have to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Time [h]    PV_Temp     PV_pO2\n",
      "0        0.001495  17.876518  73.209167\n",
      "1        0.002919  17.876518  73.233047\n",
      "2        0.004308  17.865667  73.686722\n",
      "3        0.005728  17.892794  73.209167\n",
      "4        0.007117  17.919922  73.185287\n",
      "...           ...        ...        ...\n",
      "87244  121.179309  51.003689  56.519730\n",
      "87245  121.180698  51.258678  56.635124\n",
      "87246  121.182087  51.486542  56.542809\n",
      "87247  121.183476  51.578773  56.542809\n",
      "87248  121.184865  51.795788  57.119778\n",
      "\n",
      "[87249 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from lucullus_rest.core import export_to_df\n",
    "\n",
    "auth = (\"user\", \"password\")\n",
    "process = \"Process_555\"\n",
    "port_names = [\"PV_pO2\", \"PV_Temp\"]\n",
    "\n",
    "df = export_to_df(process, port_names, auth)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples we got ALL time-value pairs, but maybe we only want the data interpolated for all 5 minutes. In this case we can do the same thing, but specify an interpolation interval of 300 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Time [h]    PV_Temp      PV_pO2\n",
      "0       0.001495  17.876518   73.209167\n",
      "1       0.084829  27.587854  103.868195\n",
      "2       0.168162  29.257763  103.898261\n",
      "3       0.251495  29.855684  104.032183\n",
      "4       0.334829  30.300564  103.415624\n",
      "...          ...        ...         ...\n",
      "1450  120.834829  22.734263   56.983755\n",
      "1451  120.918162  21.454226   56.960584\n",
      "1452  121.001495  21.604972   57.217340\n",
      "1453  121.084829  20.716512   56.804750\n",
      "1454  121.168162  49.530108   57.126532\n",
      "\n",
      "[1455 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = export_to_df(process, port_names, auth, interval=300)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted the data, we can easily save it as an excel to a folder of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"C:/Users/Public/Documents\"\n",
    "save_path = save_folder + \"/{}.xlsx\".format(process)\n",
    "df.to_excel(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to extract now the data for a number of processes, we could of course also do it, for instance like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_processes = [\"Process_555\", \"Process_556\"]\n",
    "\n",
    "for p in list_of_processes:\n",
    "    df = export_to_df(p, port_names, auth)\n",
    "    save_path = save_folder + \"/{}.xlsx\".format(p)\n",
    "    df.to_excel(save_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a5771fbb30f90b476369e73d404b5f512ca290f716812004d147ef9d0b71a39"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
